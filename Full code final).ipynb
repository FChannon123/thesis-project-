{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c002cc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "source": [
    "## Checking Volatility Estimators on S&P 2020 High Frequency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e134a78-b6e5-4c13-8598-239301cb522f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:10.534456+00:00",
     "start_time": "2023-06-23T13:12:10.372214+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "4c7562a0-013b-42b3-846b-04db6390841d"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to the dataset directory\n",
    "path = '/Users/chess/Desktop/MSc Statistics/summer thesis/Data Thesis 23 June/_data_dwn_16_88__SPY_2010-01-01_2010-12-31_1'\n",
    "\n",
    "# Get all the order book files\n",
    "order_book_files = glob.glob(os.path.join(path, '*orderbook_1.csv'))\n",
    "\n",
    "# Get all the message book files\n",
    "message_book_files = glob.glob(os.path.join(path, '*message_1.csv'))\n",
    "\n",
    "# Sort the files\n",
    "order_book_files.sort()\n",
    "message_book_files.sort()\n",
    "\n",
    "# Check the first few files\n",
    "print('Order book files:', order_book_files[:5])\n",
    "print('Message book files:', message_book_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadc5e1-c04e-4f8e-97bc-a26e8e4f92e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:10.757404+00:00",
     "start_time": "2023-06-23T13:12:10.598565+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "33df2312-b0db-47ba-96fd-5f7cb3813bc7"
    }
   },
   "outputs": [],
   "source": [
    "def process_file(order_book_file, message_book_file):\n",
    "    # Load the order book file\n",
    "    order_book = pd.read_csv(order_book_file, names=['Ask Price 1', 'Ask Size 1', 'Bid Price 1', 'Bid Size 1'])\n",
    "\n",
    "    # Load the message book file\n",
    "    message_book = pd.read_csv(message_book_file, names=['Time', 'Type', 'Order ID', 'Volume', 'Price', 'Direction', 'Null'])\n",
    "\n",
    "    # Merge the order book and message book data\n",
    "    data = pd.concat([order_book.reset_index(drop=True), message_book.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Calculate the mid price\n",
    "    data['Mid Price'] = (data['Ask Price 1'] + data['Bid Price 1']) / 2\n",
    "\n",
    "    # Calculate the log mid price\n",
    "    data['Log Mid Price'] = np.log(data['Mid Price'])\n",
    "\n",
    "    # Calculate the log returns\n",
    "    data['Log Return'] = data['Log Mid Price'].diff()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e658e-b53c-4afe-88fa-b32fc775c9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:10.920352+00:00",
     "start_time": "2023-06-23T13:12:10.763131+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "1e8ac3c6-34f1-4f2f-88c2-8da01c58c8b3"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_TV(data, n, zeta, p):\n",
    "    # Calculate the length of each increment\n",
    "    delta_n = len(data) // n\n",
    "\n",
    "    # Calculate the high-frequency log-returns\n",
    "    data['High-Frequency Log Return'] = data['Log Return'].rolling(delta_n).sum()\n",
    "\n",
    "    # Calculate the TV estimator\n",
    "    data['TV'] = (data['High-Frequency Log Return']**2) * (abs(data['High-Frequency Log Return']) < zeta * delta_n**p)\n",
    "    TV = data['TV'].sum()\n",
    "\n",
    "    return TV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18bf69a-4f49-41d4-905e-e071095ac530",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `process_file` takes the paths to an order book file and a message book file as input. This function does the following:\n",
    "\n",
    "1. Loads the order book file and the message book file.\n",
    "2. Merges the two datasets into one.\n",
    "3. Calculates the mid price, which is the average of the ask price and the bid price.\n",
    "4. Calculates the log of the mid price.\n",
    "5. Calculates the log returns, which are the differences in the log mid prices.\n",
    "\n",
    "You can use this function to process each pair of order book and message book files. The function returns a DataFrame that contains the merged and processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5172d9-1063-4b93-8072-cab4d4ae1dfb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `calculate_TV` calculates the Truncated Volatility (TV) estimator. This function takes as input a DataFrame that contains the processed data, the number of increments `n`, and the parameters `zeta` and `p` for the truncation threshold. The function does the following:\n",
    "\n",
    "1. Calculates the length of each increment.\n",
    "2. Calculates the high-frequency log-returns.\n",
    "3. Calculates the TV estimator.\n",
    "\n",
    "The function returns the TV estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444e489-9158-4381-81de-3b83628d1c2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:11.085150+00:00",
     "start_time": "2023-06-23T13:12:10.928037+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "a243f9d8-3309-4f4f-88e3-7363d882b2f1"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_DV(data, n, zeta, p):\n",
    "    # Calculate the length of each increment\n",
    "    delta_n = len(data) // n\n",
    "\n",
    "    # Calculate the high-frequency log-returns\n",
    "    data['High-Frequency Log Return'] = data['Log Return'].rolling(delta_n).sum()\n",
    "\n",
    "    # Calculate the differenced log-returns\n",
    "    data['Differenced Log Return'] = data['High-Frequency Log Return'].diff()\n",
    "\n",
    "    # Calculate the DV estimator\n",
    "    data['DV'] = 0.5 * (data['Differenced Log Return']**2) * (abs(data['Differenced Log Return']) < zeta * delta_n**p)\n",
    "    DV = data['DV'].sum()\n",
    "\n",
    "    return DV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a81cd-58ed-49f1-bfe3-6676fd5f0692",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `calculate_DV` calculates the Differenced-return Volatility (DV) estimator. This function takes as input a DataFrame that contains the processed data, the number of increments `n`, and the parameters `zeta` and `p` for the truncation threshold. The function does the following:\n",
    "\n",
    "1. Calculates the length of each increment.\n",
    "2. Calculates the high-frequency log-returns.\n",
    "3. Calculates the differenced log-returns.\n",
    "4. Calculates the DV estimator.\n",
    "\n",
    "The function returns the DV estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd5260-b381-4e95-9d89-b2b9db8578e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:11.252237+00:00",
     "start_time": "2023-06-23T13:12:11.095411+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "356ef62d-9a4b-43c2-aca6-2ad781c43436"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_DV_m(data, n, m, zeta, p):\n",
    "    # Calculate the length of each increment\n",
    "    delta_n = len(data) // n\n",
    "\n",
    "    # Calculate the high-frequency log-returns\n",
    "    data['High-Frequency Log Return'] = data['Log Return'].rolling(delta_n).sum()\n",
    "\n",
    "    # Calculate the differenced log-returns\n",
    "    data['Differenced Log Return'] = data['High-Frequency Log Return'].diff(m)\n",
    "\n",
    "    # Calculate the DV_m estimator\n",
    "    data['DV_m'] = 0.5 * (data['Differenced Log Return']**2) * (abs(data['Differenced Log Return']) <= zeta * delta_n**p)\n",
    "    DV_m = data['DV_m'].sum()\n",
    "\n",
    "    return DV_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa2302-4fde-4419-bbad-1bce68a21f01",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `calculate_DV_m` calculates the Differenced-return Volatility (DV) estimator for a given lag `m`. This function takes as input a DataFrame that contains the processed data, the number of increments `n`, the lag `m`, and the parameters `zeta` and `p` for the truncation threshold. The function does the following:\n",
    "\n",
    "1. Calculates the length of each increment.\n",
    "2. Calculates the high-frequency log-returns.\n",
    "3. Calculates the differenced log-returns with a lag of `m`.\n",
    "4. Calculates the DV_m estimator.\n",
    "\n",
    "The function returns the DV_m estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e9cf6-0536-475d-a06a-fb3d9dbb50f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:11.418399+00:00",
     "start_time": "2023-06-23T13:12:11.262066+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "786df31b-0ade-4333-bf82-d73225673aa2"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_RV(data, n):\n",
    "    # Calculate the length of each increment\n",
    "    delta_n = len(data) // n\n",
    "\n",
    "    # Calculate the high-frequency log-returns\n",
    "    data['High-Frequency Log Return'] = data['Log Return'].rolling(delta_n).sum()\n",
    "\n",
    "    # Calculate the RV estimator\n",
    "    data['RV'] = data['High-Frequency Log Return']**2\n",
    "    RV = data['RV'].sum()\n",
    "\n",
    "    return RV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d64dc1-2bbd-47a8-9f94-a79021db95e2",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `calculate_RV` calculates the Realized Volatility (RV) estimator. This function takes as input a DataFrame that contains the processed data and the number of increments `n`. The function does the following:\n",
    "\n",
    "1. Calculates the length of each increment.\n",
    "2. Calculates the high-frequency log-returns.\n",
    "3. Calculates the RV estimator.\n",
    "\n",
    "The function returns the RV estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a4a95-ff6b-4f8b-ba85-813c8fcdcf83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:11.583917+00:00",
     "start_time": "2023-06-23T13:12:11.426630+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "48007688-6de6-43f0-964d-8721b4215736"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_DV_1n_M(data, n, M, zeta, p):\n",
    "    # Calculate the DV_m estimators for m = 1, 2, ..., M\n",
    "    DV_m_values = [calculate_DV_m(data, n, m, zeta, p) for m in range(1, M+1)]\n",
    "\n",
    "    # Calculate the Averaged DV estimator\n",
    "    DV_1n_M = sum(DV_m_values) / M\n",
    "\n",
    "    return DV_1n_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c87e6-048c-4836-80f6-d4e71d4d2792",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `calculate_DV_1n_M` calculates the Averaged DV estimator. This function takes as input a DataFrame that contains the processed data, the number of increments `n`, the maximum lag `M`, and the parameters `zeta` and `p` for the truncation threshold. The function does the following:\n",
    "\n",
    "1. Calculates the DV_m estimators for m = 1, 2, ..., M.\n",
    "2. Calculates the Averaged DV estimator.\n",
    "\n",
    "The function returns the Averaged DV estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094151a2-2018-47b8-80f2-6c38bfd14e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:12:11.752269+00:00",
     "start_time": "2023-06-23T13:12:11.591476+00:00"
    },
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "23cac930-6764-42cd-a9ce-6e02d0aec089"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_volatility_estimates(order_book_file, message_book_file, n, M, zeta, p):\n",
    "    # Process the data\n",
    "    data = process_file(order_book_file, message_book_file)\n",
    "\n",
    "    # Calculate the volatility estimators\n",
    "    RV = calculate_RV(data, n)\n",
    "    TV = calculate_TV(data, n, zeta, p)\n",
    "    DV = calculate_DV(data, n, zeta, p)\n",
    "    DV_m = calculate_DV_m(data, n, M, zeta, p)\n",
    "    DV_1n_M = calculate_DV_1n_M(data, n, M, zeta, p)\n",
    "\n",
    "    # Create a DataFrame to store the volatility estimates\n",
    "    volatility_estimates = pd.DataFrame({\n",
    "        'RV': [RV],\n",
    "        'TV': [TV],\n",
    "        'DV': [DV],\n",
    "        'DV_m': [DV_m],\n",
    "        'DV_1n_M': [DV_1n_M]\n",
    "    })\n",
    "\n",
    "    return volatility_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90624f7e-2a47-4d59-95eb-a0fa63bfa4ab",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `calculate_volatility_estimates` takes the paths to an order book file and a message book file, processes the data, and calculates all the volatility estimators. This function does the following:\n",
    "\n",
    "1. Processes the data using the `process_file` function.\n",
    "2. Calculates the RV estimator using the `calculate_RV` function.\n",
    "3. Calculates the TV estimator using the `calculate_TV` function.\n",
    "4. Calculates the DV estimator using the `calculate_DV` function.\n",
    "5. Calculates the DV_m estimator for a given lag `m` using the `calculate_DV_m` function.\n",
    "6. Calculates the Averaged DV estimator using the `calculate_DV_1n_M` function.\n",
    "\n",
    "The function returns a DataFrame that contains the volatility estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b79f6d-b355-4497-aa35-726ab2c6b055",
   "metadata": {
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "9401c0e1-61c2-4b43-b71e-c161c54fb5e6"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_volatility_estimates_for_all_files(order_books, message_books, n, M, zeta, p):\n",
    "    #Initialize an empty DataFrame to store the volatility estimates\n",
    "    volatility_estimates = pd.DataFrame()\n",
    "    \n",
    "    # Process each pair of order book and message book files\n",
    "    for i in range(0, len(order_books)):\n",
    "        # Get the paths to the order book file and the message book file\n",
    "        order_book_file = order_books[i]\n",
    "        message_book_file = message_books[i]\n",
    "        \n",
    "        # Get date from file name\n",
    "        date_order = datetime.strptime(order_book_file, \"/Users/chess/Desktop/MSc Statistics/summer thesis/Data Thesis 23 June/_data_dwn_16_88__SPY_2010-01-01_2010-12-31_1/SPY_%Y-%m-%d_34200000_57600000_orderbook_1.csv\")\n",
    "        date_message = datetime.strptime(message_book_file, \"/Users/chess/Desktop/MSc Statistics/summer thesis/Data Thesis 23 June/_data_dwn_16_88__SPY_2010-01-01_2010-12-31_1/SPY_%Y-%m-%d_34200000_57600000_message_1.csv\")\n",
    "        if date_order != date_message:\n",
    "            raise IndexError('dates do not match')\n",
    "        \n",
    "        # Calculate the volatility estimates for the day\n",
    "        daily_volatility_estimates = calculate_volatility_estimates(order_book_file, message_book_file, n, M, zeta, p)\n",
    "\n",
    "        # Add the date to the DataFrame\n",
    "        daily_volatility_estimates['Date'] = date_order\n",
    "\n",
    "        # Append the daily volatility estimates to the DataFrame\n",
    "        volatility_estimates = volatility_estimates.append(daily_volatility_estimates, ignore_index=True)\n",
    "\n",
    "    # Set the date as the index of the DataFrame\n",
    "    volatility_estimates.set_index('Date', inplace=True)\n",
    "\n",
    "    return volatility_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655db34-10b5-4b44-bf37-c25e69d8b634",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "The function `calculate_volatility_estimates_for_all_files` takes the path to the directory that contains the order book and message book files, processes each pair of files, and calculates the volatility estimates. This function does the following:\n",
    "\n",
    "1. Initializes an empty DataFrame to store the volatility estimates.\n",
    "2. Gets the list of files in the directory.\n",
    "3. Sorts the files by date.\n",
    "4. Processes each pair of order book and message book files.\n",
    "5. Calculates the volatility estimates for each day using the `calculate_volatility_estimates` function.\n",
    "6. Adds the date to the DataFrame.\n",
    "7. Appends the daily volatility estimates to the DataFrame.\n",
    "\n",
    "The function returns a DataFrame that contains the volatility estimates for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8ddb5-5058-4909-b7f9-b5df78e08a66",
   "metadata": {
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "7f8bf0b5-f22f-4e37-8fea-2dc0a72fab72"
    }
   },
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "n = 78\n",
    "M = 78\n",
    "zeta = 3\n",
    "p = 0.4\n",
    "\n",
    "# Calculate the volatility estimates for all the files\n",
    "volatility_estimates = calculate_volatility_estimates_for_all_files(order_book_files, message_book_files, n, M, zeta, p)\n",
    "print(volatility_estimates)\n",
    "# Plot the volatility estimates\n",
    "volatility_estimates.plot(figsize=(10, 6))\n",
    "plt.title('Volatility Estimates')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8521bba-6145-43e9-b435-a3331c6c9490",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "In the above code block, we set the parameters `n`, `M`, `zeta`, and `p` as per the paper's specifications. We then set the path to the directory that contains the order book and message book files.\n",
    "\n",
    "We call the function `calculate_volatility_estimates_for_all_files` with the directory and parameters as arguments. This function processes each pair of order book and message book files in the directory, calculates the volatility estimates for each day, and returns a DataFrame that contains the volatility estimates for each day.\n",
    "\n",
    "Finally, we plot the volatility estimates over time. The x-axis represents the date, and the y-axis represents the volatility. Each line in the plot corresponds to a different volatility estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862fe81-f5a1-422c-a79d-a6629c36e479",
   "metadata": {
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "0363e4cb-614e-402b-8f57-05b61adc687c"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of each volatility estimator\n",
    "mean_volatility_estimates = volatility_estimates.mean()\n",
    "std_volatility_estimates = volatility_estimates.std()\n",
    "\n",
    "# Calculate the correlation between the different volatility estimators\n",
    "correlation_matrix = volatility_estimates.corr()\n",
    "\n",
    "# Print the results\n",
    "print('Mean Volatility Estimates:')\n",
    "print(mean_volatility_estimates)\n",
    "print('\\nStandard Deviation of Volatility Estimates:')\n",
    "print(std_volatility_estimates)\n",
    "print('\\nCorrelation Matrix:')\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c02db-95b8-4f6a-8cd2-d9925705eeb9",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "In the above code block, we calculate the mean and standard deviation of each volatility estimator. This gives us an idea of the average level of volatility and the variability of the volatility estimates.\n",
    "\n",
    "We also calculate the correlation between the different volatility estimators. This gives us an idea of how the estimators relate to each other. A high positive correlation indicates that the estimators tend to move in the same direction, while a high negative correlation indicates that they tend to move in opposite directions.\n",
    "\n",
    "Finally, we print the results. The mean and standard deviation are printed for each estimator, and the correlation matrix is printed to show the correlations between the different estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0473af-fd54-435c-b027-09f8c6a81e3e",
   "metadata": {
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "2523f3e4-3d33-4500-800a-5c51eb4a509b"
    }
   },
   "outputs": [],
   "source": [
    "# Compare the mean volatility estimates to the findings in the paper\n",
    "print('Mean Volatility Estimates compared to the findings in the paper:')\n",
    "print(mean_volatility_estimates)\n",
    "\n",
    "# Compare the standard deviation of the volatility estimates to the findings in the paper\n",
    "print('\\nStandard Deviation of Volatility Estimates compared to the findings in the paper:')\n",
    "print(std_volatility_estimates)\n",
    "\n",
    "# Compare the correlation matrix to the findings in the paper\n",
    "print('\\nCorrelation Matrix compared to the findings in the paper:')\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7e761-fb5e-4ad9-a3db-fe5fde9fc0eb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "In the above code block, we compare the mean, standard deviation, and correlation of the volatility estimates to the findings in the paper. This allows us to see if our results are consistent with the paper's findings and to identify any differences.\n",
    "\n",
    "We print the mean, standard deviation, and correlation of the volatility estimates, and we compare these statistics to the corresponding statistics in the paper.\n",
    "\n",
    "By comparing our results to the paper's findings, we can validate our implementation of the volatility estimators and gain insights into the properties of the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81477dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Specify the column names\n",
    "message_book_columns = ['Time', 'Type', 'Order ID', 'Volume', 'Price', 'Direction', 'null']\n",
    "order_book_columns = ['Ask Price 1', 'Ask Size 1', 'Bid Price 1', 'Bid Size 1']\n",
    "\n",
    "# Load the CSV files into pandas DataFrames\n",
    "message_book = pd.read_csv('/Users/chess/Desktop/MSc Statistics/summer thesis/Data Thesis 23 June/_data_dwn_16_88__SPY_2010-01-01_2010-12-31_1/SPY_2010-05-06_34200000_57600000_message_1.csv', names=message_book_columns)\n",
    "order_book = pd.read_csv('/Users/chess/Desktop/MSc Statistics/summer thesis/Data Thesis 23 June/_data_dwn_16_88__SPY_2010-01-01_2010-12-31_1/SPY_2010-05-06_34200000_57600000_orderbook_1.csv', names=order_book_columns)\n",
    "\n",
    "# Convert time column to datetime format with the specified date as the origin\n",
    "message_book['Time'] = pd.to_datetime(full_data['Time'], unit='s', origin=desired_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a candlestick chart\n",
    "fig = go.Figure(data=go.Scatter(x=message_book['Time'], y=full_data['Price'], line=dict(color='blue')))\n",
    "\n",
    "# Set x-axis range to show the time range from 9:30 AM to 3:59 PM\n",
    "start_time = pd.to_datetime(desired_date + ' 09:31:00')\n",
    "end_time = pd.to_datetime(desired_date + ' 15:30:00')\n",
    "fig.update_layout(xaxis_range=[start_time, end_time])\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title='Price Trend',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Price',\n",
    "    plot_bgcolor='white',\n",
    "    autosize=True,\n",
    "    width=1200,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d2835-4344-461a-b6b8-6d6f9a274ffb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "In the above code block, we load the message book file for 6th May 2010. We convert the 'Time' column to datetime and filter the data to include only the time period from 9:00 to 15:00.\n",
    "\n",
    "We then plot the price of the S&P 500 over this time period. The x-axis represents the time, and the y-axis represents the price. The plot shows the price dynamics of the S&P 500 on 6th May 2010, including the flash crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d75e1-6ceb-4f20-8b7e-e609ec75962e",
   "metadata": {
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "bd995d1d-f54b-477e-aaee-e15cb22e0be7"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the daily RV measured at different frequencies\n",
    "rv_1m = calculate_RV(volatility_estimates, frequency='1min')\n",
    "rv_5m = calculate_RV(volatility_estimates, frequency='5min')\n",
    "rv_10m = calculate_RV(volatility_estimates, frequency='10min')\n",
    "rv_20m = calculate_RV(volatility_estimates, frequency='20min')\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "rv_df = pd.DataFrame({'RV_1m': rv_1m, 'RV_5m': rv_5m, 'RV_10m': rv_10m, 'RV_20m': rv_20m})\n",
    "\n",
    "# Print the results\n",
    "print('Table 1: Daily RV measured at different frequencies')\n",
    "print(rv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cc594-f854-49a0-aa9a-1c66c475c04c",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "In the above code block, we calculate the daily RV measured at different frequencies (1 minute, 5 minutes, 10 minutes, and 20 minutes). This gives us an idea of the volatility of the asset at different time scales.\n",
    "\n",
    "We then create a DataFrame for the results and print the DataFrame. The DataFrame shows the RV measured at different frequencies for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7b935-ccd8-4749-9a16-9fe88984b83b",
   "metadata": {
    "noteable": {
     "cell_type": "code",
     "output_collection_id": "16c99760-efac-4bc7-8c99-0322f38197b6"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the TV and DV estimates for each day\n",
    "tv_estimates = calculate_TV(volatility_estimates)\n",
    "dv_estimates = calculate_DV(volatility_estimates)\n",
    "\n",
    "# Calculate the t-statistic for presence of extreme return persistence\n",
    "t_stat = calculate_t_stat(tv_estimates, dv_estimates, rv_df)\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "volatility_estimates_df = pd.DataFrame({'TV': tv_estimates, 'DV': dv_estimates, 'T_stat': t_stat})\n",
    "\n",
    "# Print the results\n",
    "print('Table 2: Volatility estimates for each day')\n",
    "print(volatility_estimates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf68d1-cfcb-4777-a5c1-35d2ae08a5d2",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "In the above code block, we calculate the TV and DV estimates for each day. The TV estimate is used when the observed prices do not contain a persistent noise component, and the DV estimate is used when the observed prices contain a persistent noise component.\n",
    "\n",
    "We also calculate the t-statistic for the presence of extreme return persistence. This statistic tests the null hypothesis that the TV and DV estimates are equal. A large absolute value of the t-statistic indicates that the null hypothesis is likely to be rejected, suggesting the presence of extreme return persistence.\n",
    "\n",
    "We then create a DataFrame for the results and print the DataFrame. The DataFrame shows the TV and DV estimates and the t-statistic for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c9fa0-161c-4675-af24-c7f479d758f6",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set the Google Cloud Storage parameters\n",
    "bucket_name = 'thesis_dataset_fc98'\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "# Set the path to the data files\n",
    "data_path = '/Users/chess/Desktop/MSc Statistics/summer thesis/Data Thesis 23 June/_data_dwn_16_88__SPY_2010-01-01_2010-12-31_1/'\n",
    "\n",
    "# Set the names of the data files\n",
    "order_book_file_name = 'SPY_2010-05-06_34200000_57600000_orderbook_1.csv'\n",
    "message_book_file_name = 'SPY_2010-05-06_34200000_57600000_message_1.csv'\n",
    "\n",
    "# Set the paths to the data files\n",
    "order_book_file_path = os.path.join(data_path, order_book_file_name)\n",
    "message_book_file_path = os.path.join(data_path, message_book_file_name)\n",
    "\n",
    "# Download the data files from the Google Cloud Storage bucket\n",
    "bucket.blob(order_book_file_name).download_to_filename(order_book_file_path)\n",
    "bucket.blob(message_book_file_name).download_to_filename(message_book_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "noteable": {
   "last_delta_id": "ea1b43e3-dff0-4c66-ba34-f6d73dc64107",
   "last_transaction_id": "c391a488-fa95-4461-96c2-df3ecfaf7b47"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "7590a9ad-9526-5e36-ad3b-db0f8721ed70",
    "openai_ephemeral_user_id": "ea877551-d184-540a-94ae-8d14165aa509",
    "openai_subdivision1_iso_code": "GB-ENG"
   }
  },
  "nteract": {
   "version": "noteable@2.9.0"
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
